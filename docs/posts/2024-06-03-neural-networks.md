---
title: 神经网络
date: 2024-06-03
tags:
    - 深度学习
---

本篇旨在简单的介绍神经网络的基础知识概念,不会过多涉及具体实现与细节问题

---

::: details 目录

[[toc]]

:::


## 基本概念

不得不说深度学习的专有名词很多,实际上可能没有想象中的那么复杂,这里引用Eric Xing的一句话:

> People in the deep learning community love to invent new words  -- Eric Xing

下面是一些和本篇博客相关的前置基础概念

- 多层感知机(MLP)
- 隐藏层
- 权重与偏置
- 链式法则


## 梯度下降

我们利用梯度下降法来调整神经网络中的权重与偏置

梯度下降会默认一个初始值,然后尝试向最优方向走(依据损失函数)

一般来说根据损失函数的导数,导数越大我们下降的步长(step size)越大,越接近答案则步长越小[^1]

更严谨的说,梯度下降即为:

当我们有一个函数的多个导数的时候[^2],我们称之为梯度,我们会利用这些梯度下降到损失函数的最低点

这里一般我们会利用`导数*学习率`来确定步长的大小[^3]

当步长很小的时候我们会认为接近答案,停止梯度下降

当然一般我们也会设置最大步数,如果超过就停止

:::tip 题外话

梯度下降用于寻找最优的参数,但是你可以会联想到线性回归中我们在高中也学过类似的方法

但是我们用的是最小二乘法,并且最小二乘似乎可以更快的求出最优的参数解

为什么不用最小二乘法而是用梯度下降呢?

因为适用性有限, 最小二乘必须要知道导数为0的点

很多时候不能解出导数为0的点,梯度下降则更为适用

:::

## 反向传播

BackPropagation(BP算法),某种意义上就是链式法则,但是在这里我们不需要对每个参数都求一遍偏导  

因为前面的层一定用的上后面的层的结果,所以反向传播可以减少计算(某种意义上来说的DP)


## 激活函数

激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算

### ReLU

最常见的激活函数之一,又名修正线性单元

实现上也很简洁:

$$ReLU(x)=max(x,0)$$

使用ReLU的原因是，它求导表现得特别好：

要么让参数消失，要么让参数通过。 

这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题(sigmod)

### Sigmoid

将输入变换为区间(0, 1)上的输出

$$sigmod(x)=\frac{1}{1+\exp(-x)}$$

当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数(这里sigmoid可以视为softmax的特例)

然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代(可能会梯度消失)。

### SoftMax

一般来说SoftMax会作为分类问题的输出单元上的激活函数[^4]

SoftMax可以将输出层的结果归一化为0到1之间的数值[^5]

SoftMax的公式为:

$$y_i=\frac{e^{x_i}}{\sum^{N}_{j=1}e^{x_j}}$$

### Tanh

双曲正切会将输入压缩转换到区间(-1, 1)上:

$$tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}$$


## 损失函数

损失函数（loss function）能够量化目标的实际值与预测值之间的差距。   

通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0

### 残差平方和

回归问题中最常用的损失函数是残差平方和(即平方误差函数)


### 交叉熵

> 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数

#### 为什么使用?

这里是一个比较随意的解释:

比如SoftMax将数值变0到1之间,对于实际为1,分类的0.1的情况,原先的残差平方和惩罚力度不够(导数变化小)会导致梯度下降的时候步长较大

而对数对应值很小的时候会膨胀的很快,因此对于分类问题,为了增加对于低预测值的惩罚力度,我们会使用交叉熵

从更严谨的角度来说,我们实际上在最大化观测数据的似然[^6]

#### 更有趣一点?

我们可以从一个更有趣的角度理解交叉熵,详见[信息论与交叉熵](/posts/2024-06-05-cross-entropy)







## 更多注释

[^1]: 严格意义上是偏导

[^2]: 因为对每个自变量求偏导都会有一个导数

[^3]: 尽管梯度下降对学习率是敏感的,在实际中我们一般会通过验证集调整学习率等超参数,所以一般不用太过于在意学习率的取值

[^4]: 对于二分类通常使用sigmoid即可

[^5]: 对于一个分类问题某种意义上这可以视为这对应类别的概率,这是这种概率并不是很可靠的(尽管神经网络的效果可能很不错)

[^6]: 最大化观测数据的似然相当于最小化负对数似然