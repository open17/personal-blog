---
title: 深度学习的基本数学知识(一)
date: 2024-06-05
tags:
    - 数学
    - 深度学习
    - 线性代数
---


这里只介绍最简单的深度学习涉及的基础的数学内容

---

[[toc]]

## 线性代数与基本运算

### 标量,向量,矩阵与张量

我们可以简单的把后者视为前者的推广


### 哈达玛积

假设矩阵$A$和$B$都是$m \times n$维的矩阵，其元素分别表示为$A_{ij}$和$B_{ij}$

我们把两个矩阵按元素相乘称之为Hadamard product,表示为$A\odot B$

$A$和$B$的Hadamard积$C$的元素$C_{ij}$可以表示为

$$C_{ij} = A_{ij} \times B_{ij}$$

### 点积

对于n维向量x与y,其点积表示为$<x,y>$或者$x^Ty$[^1]

点积是对于**向量**的相同位置的按元素乘积的和:

$$x^Ty=\sum_{i}^nx_iy_i$$

### 矩阵-向量积

$m \times n$的矩阵$A$可以表示为由$m$个行向量$a_i^T$组成，其中每个$a_i \in \mathbb{R}^n$:
$$A = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_m^T \end{bmatrix}$$

对于一个n维向量x对矩阵A做矩阵向量积:

$$Ax = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_m^T \end{bmatrix}x = \begin{bmatrix} a_1^T x \\ a_2^T x \\ \vdots \\ a_m^T x \end{bmatrix}$$

$Ax$的结果是一个$m$维向量，其中每个元素是对应行向量$a_i^T$和$x$的点积

因此我们可以把矩阵向量积视为一种将$n$维向量转为$m$维向量的变换


### 矩阵-矩阵乘法

设A为$m \times p$的矩阵,B为$p\times n$的矩阵

我们可以将矩阵-矩阵乘法看作简单地执行$n$次矩阵-向量积，并将结果拼接在一起，形成一个$n\times m$矩阵

$$AB = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_m^T \end{bmatrix}\begin{bmatrix} b_1 \space b_2 \space \dots  \space b_n \end{bmatrix} = \begin{bmatrix}
a_1^T b_1 & a_1^T b_2 & \cdots & a_1^T b_n \\
a_2^T b_1 & a_2^T b_2 & \cdots & a_2^T b_n \\
\vdots & \vdots & \ddots & \vdots \\
a_m^T b_1 & a_m^T b_2 & \cdots & a_m^T b_n
\end{bmatrix}
$$

## 线性代数与范数基础

> 在深度学习中，我们经常试图解决优化问题：   
> 
> 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。   
> 
> 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。   
>   
> 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数  
> 
> -- ***动手学习深度学习***

### 定义与性质

向量的范数（Norm）是一个函数，用于度量向量空间中向量的“长度”或“大小”

一般范数需要满足一些基本性质[^2]:
- 非负
- 绝对值缩放
- 三角不等式

范数有很多形式,在深度学习中我们一般使用L2范数的平方与L1范数进行衡量"大小"

### L1范数

相对于L2受异常值的影响较小

曼哈顿范数,向量各分量绝对值之和

$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|
$$

### L2范数

欧几里得范数,向量各分量平方和的平方根，即常规意义上的向量长度
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$

### Lp范数

L1和L2都是Lp的特例

对于 $p \geq 1$，向量 $\mathbf{x}$ 的Lp范数定义为
$$
\|\mathbf{x}\|_p = \left( \sum_{i=1}^n |x_i|^p \right)^{\frac{1}{p}}
$$
其中 $x_i$ 是向量 $\mathbf{x}$ 的第 i 个分量，$n$ 是向量的维数

### 无穷范数

切比雪夫范数,向量各分量绝对值的最大值

$$
\|\mathbf{x}\|_\infty = \max_{i=1}^n |x_i|
$$

### Frobenius范数

类似L2范数,不过定义在矩阵意义上

$$
\|\mathbf{x}\|_F = \sqrt{\sum_{i=1}^n\sum_{j=1}^m x_{ij}^2}
$$

## 注释

[^1]: 这里使用$x^Ty$而不是$xy$是因为我们默认$x$,$y$中一个为行向量一个为列向量,后面涉及到的点积公式转置的原因也都相同

[^2]: 非正式范数可能不满足以上性质,但依然能在某些情况下较好的度量向量空间中向量的“长度”或“大小”