---
title: AlexNet
date: 2024-06-06
tags:
    - 论文
    - 深度学习
    - CNN
---
    
Imagenet classification with deep convolutional neural networks.

---

AlexNet这篇论文读取来严格意义上比较偏向于技术报告而非论文,很多地方其实写的不那么像论文,但是没关系,因为它即work又有启发意义

[论文PDF地址](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)


## 引入

如果说LeNet是第一个经典的CNN架构,那么AlexNet就是第一个大放光彩的CNN架构

AlexNet是一个用于图像识别的卷积神经网络,使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛冠军

在这以前,在绝大多数时候当时的人们会认为机器学习(SVM为主)的图形识别会远远优于深度学习

当时的机器学习流程大致如下:
- 根据人们的先验知识,手工对特征数据集进行预处理
- 抽图形的SIFT/SURF等特征
- 然后分类器巴拉巴拉的

与这截然相反的,AlexNet是端到端的(直接在原始像素上做),它首次证明了学习到的特征可以超越手工设计的特征

## 架构

基础的CNN概念就不再赘述了,这里主要讲和最经典的CNN(LeNet)的一些区别

### 激活函数

与LeNet不同,AlexNet使用ReLU而非sigmod

值得一提的是论文中使用ReLU的原因是为了提高速度,但这一点从现在的角度来看不一定准确

目前不使用sigmod时主要是为了避免sigmod的梯度消失

ReLU如今被大量的使用,但最大的原因应该是简单好记好实现

### 局部响应归一化层

在激活层后套了LRN,不过LRN现在用的似乎很少了

### 池化层

与LeNet不同,这是用的是最大池化,减低对噪声的敏感

同时这里令池化窗口大于步长(重叠池化),有助于提高网络的性能


### 模型并行

值得一提的是论文也花费了许多笔墨在与将模型切开并行训练(当时作者只有两块3GB的GTX580)

这其实是比较工程的东西,所以在较长的一段时间没有很多的关注

但是随着近几年的bert等NLP模型,模型并行在自然语言处理方面逐渐成为了一大需求

## 降低过拟合

- data augmentation
  - 镜像与旋转
  - 随机剪裁
  - 变化颜色通道(用了个PCA)
- dropout

值得一提的是当时我们认为dropout类似于模型融合的操作,所以能增强泛用性,不过现在我们更多认为dropout会等价于一个L2正则(尽管不能实际表达出公式)

当然现在CNN的设计通常不会使用那么大的全连接层，所以dropout也不那么重要,不过RNN和注意力这块用的还是比较多的

## 训练细节

### 使用SGD(随机梯度下降)

在此之前因为SGD参数调整比较麻烦,使用的比较少

后来发现其对噪声表现比较好

### 动量法

病态曲率会影响SGD的最优寻找

我们对与非常不平滑的优化表面保存一定旧有的动量(保留旧的梯度并取梯度步长的指数平均),避免陷入局部最优干扰梯度的正常下降

### 学习率

AlexNet采用手动利用验证集降10倍调整

现在我们一般不怎么做,现在比较主流的做法是先做学习率预热然后平滑函数下降学习率